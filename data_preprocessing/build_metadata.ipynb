{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylangacq\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 208/1390 [05:12<27:27,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/aphasia/dataset/all_chas\\Fridriksson-2\\1003-3.cha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1390/1390 [35:36<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "all_chas = glob('/workspace/dataset/all_chas/*/*.cha') # change the path to the directory where all the .cha files are stored\n",
    "chats = []\n",
    "\n",
    "for cha in tqdm(all_chas):\n",
    "    try:\n",
    "        chat = pylangacq.read_chat(cha)\n",
    "        chats.append(chat)\n",
    "    except:\n",
    "        print(cha) # You need to check the files that are printed here and see if they are valid .cha files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1389/1389 [00:00<00:00, 4157.22it/s]\n"
     ]
    }
   ],
   "source": [
    "total_info = {}\n",
    "\n",
    "for reader in tqdm(chats):\n",
    "    info = {}\n",
    "    headers = reader.headers()[0]\n",
    "\n",
    "    # Extracting information of the participants\n",
    "    par_info = headers['Participants']['PAR']\n",
    "    chat_name = f\"{par_info['corpus']}/{headers['Media'].split(',')[0]}\"\n",
    "    \n",
    "    info['lang'] = headers['Languages'][0]\n",
    "    info['age'] = par_info['age']\n",
    "    info['sex'] = par_info['sex']\n",
    "    info['media_type'] = headers['Media'].split(',')[1].strip()\n",
    "    \n",
    "    # Extracting aphasia type and label\n",
    "    aphasia_type = par_info['group'].lower()\n",
    "    info['aphasia_type'] = aphasia_type\n",
    "    label_mapping = {\n",
    "        'control': 'Control',\n",
    "        'anomic': 'Fluent', 'conduction': 'Fluent',\n",
    "        'transsensory': 'Non-Comprehension', 'wernicke': 'Non-Comprehension',\n",
    "        'transmotor': 'Non-Fluent', 'broca': 'Non-Fluent'\n",
    "    }\n",
    "    info['label'] = label_mapping.get(aphasia_type, None)\n",
    "\n",
    "    # Cinderella task extraction\n",
    "    cinderella_task = reader.utterances(task='Cinderella')\n",
    "\n",
    "    if not cinderella_task:\n",
    "        info['timestamp'] = []\n",
    "        info['timestamp_inv'] = []\n",
    "    else:\n",
    "        # Cinderella task time extraction\n",
    "        start_time = next((u.time_marks[0] for u in cinderella_task if u.time_marks), None)\n",
    "        end_time = next((u.time_marks[1] for u in reversed(cinderella_task) if u.time_marks), None)\n",
    "\n",
    "        if start_time and end_time:\n",
    "            info['timestamp'] = (start_time, end_time)\n",
    "        \n",
    "        info['timestamp_inv'] = [utterance.time_marks for utterance in cinderella_task if utterance.participant == 'INV']\n",
    "\n",
    "    total_info[chat_name] = info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path to the directory where you want to save the metadata.json file\n",
    "with open('/workspace/RAPID/data_preprocessing/metadata.json', 'w') as f: \n",
    "    json.dump(total_info, f, ensure_ascii=False, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anomic': 285, 'broca': 309, 'conduction': 149, 'control': 367, 'transmotor': 14, 'transsensory': 2, 'wernicke': 63}\n"
     ]
    }
   ],
   "source": [
    "labels = [v['aphasia_type'] for k,v in total_info.items() if v['label'] != None]\n",
    "\n",
    "import numpy as np\n",
    "labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "print(dict(zip(labels, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cinderella task segment extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cut_video(input_video, output_video, start_time_ms, end_time_ms):\n",
    "    start_time_sec = start_time_ms / 1000.0\n",
    "    end_time_sec = end_time_ms / 1000.0\n",
    "\n",
    "    try:\n",
    "        process = (\n",
    "            ffmpeg\n",
    "            .input(input_video, ss=start_time_sec, to=end_time_sec)\n",
    "            .output(output_video, codec='copy')\n",
    "            .run_async(pipe_stdout=True, pipe_stderr=True)\n",
    "        )\n",
    "        stdout, stderr = process.communicate()\n",
    "        if process.returncode != 0:\n",
    "            raise ffmpeg.Error(f'Error cutting video {input_video}: {stderr.decode(\"utf-8\")}')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to cut video {input_video}: {str(e)}\")\n",
    "\n",
    "\n",
    "def generate_video_clips(dataset_root, metadata_path, output_dir):\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "        metadata = {k: v for k, v in metadata.items() if v['label'] is not None and v['media_type'] == 'video' and len(v['timestamp']) != 0}\n",
    "\n",
    "    video_metadata = {}\n",
    "\n",
    "    for k, v in tqdm(metadata.items(), leave=False):\n",
    "        if v['label'] is None or v['media_type'] != 'video':\n",
    "            continue\n",
    "\n",
    "        video_name = k.split(\"/\")[1] + '.mp4'\n",
    "        input_video = os.path.join(dataset_root, video_name)\n",
    "        output_video = os.path.join(output_dir, video_name)\n",
    "\n",
    "        if os.path.exists(output_video):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            start_time, end_time = v['timestamp'][0], v['timestamp'][1]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        cut_video(input_video, output_video, start_time, end_time)\n",
    "\n",
    "        try:\n",
    "            timestamp_inv = [[ts[0] - start_time, ts[1] - start_time] for ts in v.get('timestamp_inv', [])]\n",
    "        except:\n",
    "            timestamp_inv = []\n",
    "\n",
    "        video_metadata[k] = {\n",
    "            'video_name': video_name,\n",
    "            'label': v['label'],\n",
    "            'age': v['age'],\n",
    "            'lang': v['lang'],\n",
    "            'aphasia_type': v.get('aphasia_type'),\n",
    "            'timestamp_inv': timestamp_inv,\n",
    "            'duration': end_time - start_time\n",
    "        }\n",
    "\n",
    "    return video_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to cut video D:/aphasia/dataset/videos\\1030-1.mp4: __init__() missing 2 required positional arguments: 'stdout' and 'stderr'\n",
      "Failed to cut video D:/aphasia/dataset/videos\\1030-5.mp4: __init__() missing 2 required positional arguments: 'stdout' and 'stderr'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dataset_root = '/workspace/dataset/videos'\n",
    "metadata_path = '/workspace/RAPID/data_preprocessing/metadata.json'\n",
    "output_dir = \"/workspace/dataset/video_clips\"\n",
    "\n",
    "video_metadata = generate_video_clips(dataset_root, metadata_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anomic': 266, 'broca': 287, 'conduction': 138, 'control': 328, 'transmotor': 13, 'transsensory': 1, 'wernicke': 59}\n"
     ]
    }
   ],
   "source": [
    "labels = [v['aphasia_type'] for k,v in video_metadata.items() if v['label'] != None]\n",
    "\n",
    "import numpy as np\n",
    "labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "print(dict(zip(labels, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspace/RAPID/data_preprocessing/video_metadata.json', 'w') as f:\n",
    "    json.dump(video_metadata, f, ensure_ascii=False, indent=4, sort_keys=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aphasia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
