{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(aphasia_type):\n",
    "   \"\"\"\n",
    "   aphasia_type에 따른 여러 레이블을 반환하는 함수\n",
    "   \n",
    "   Parameters:\n",
    "       aphasia_type (str): WAB 기반 실어증 타입\n",
    "       \n",
    "   Returns:\n",
    "       type (str): 표준화된 실어증 타입명\n",
    "       status (str): Control 또는 Aphasia 상태\n",
    "       ct_label (int): Control(0) vs Treatment(1) 레이블\n",
    "       wab_label (int): Western Aphasia Battery 기반 레이블\n",
    "       type_label (int): Control(0), Fluent(1), Non-Comprehension(2), Non-Fluent(3)\n",
    "       flu_label (int): Non-Fluent(0) vs Fluent(1)  \n",
    "       com_label (int): Comprehends(0) vs Not Comprehends(1)\n",
    "   \"\"\"\n",
    "   \n",
    "   # 소문자로 변환하여 처리\n",
    "   aphasia_type = aphasia_type.lower()\n",
    "   \n",
    "   # 기본값 설정\n",
    "   type = \"\"\n",
    "   ct_label = 0\n",
    "   wab_label = 0 \n",
    "   type_label = 0\n",
    "   flu_label = 0\n",
    "   com_label = 0\n",
    "   \n",
    "   if aphasia_type in ['control', 'notaphasicbywab']:\n",
    "       type = \"Control\"\n",
    "       status = 'Control'\n",
    "       ct_label = 0\n",
    "       wab_label = 0\n",
    "       type_label = 0\n",
    "       flu_label = 0\n",
    "       com_label = 0\n",
    "       \n",
    "   elif aphasia_type in ['anomic', 'conduction']:\n",
    "       type = aphasia_type.capitalize()\n",
    "       status = 'Aphasia'\n",
    "       ct_label = 1\n",
    "       wab_label = 1  # Fluent & Comprehends\n",
    "       type_label = 1  # Fluent\n",
    "       flu_label = 1  # Fluent\n",
    "       com_label = 0  # Comprehends\n",
    "       \n",
    "   elif aphasia_type in ['wernicke', 'transsensory']:\n",
    "       type = \"Wernicke\" if aphasia_type == 'wernicke' else \"Trans. Sensory\"\n",
    "       status = 'Aphasia'\n",
    "       ct_label = 1\n",
    "       wab_label = 2  # Fluent & Not Comprehends\n",
    "       type_label = 2  # Non-Comprehension\n",
    "       flu_label = 1  # Fluent\n",
    "       com_label = 1  # Not Comprehends\n",
    "       \n",
    "   elif aphasia_type in ['broca', 'transmotor']:\n",
    "       type = \"Broca\" if aphasia_type == 'broca' else \"Trans. Motor\"\n",
    "       status = 'Aphasia'\n",
    "       ct_label = 1\n",
    "       wab_label = 3  # Non-Fluent & Comprehends\n",
    "       type_label = 3  # Non-Fluent\n",
    "       flu_label = 0  # Non-Fluent\n",
    "       com_label = 0  # Comprehends\n",
    "       \n",
    "   elif aphasia_type in ['global', 'isolation']:\n",
    "       type = \"Global\" if aphasia_type == 'global' else \"Isolation\"\n",
    "       status = 'Aphasia'\n",
    "       ct_label = 1\n",
    "       wab_label = 4  # Non-Fluent & Not Comprehends\n",
    "       type_label = 3  # Non-Fluent\n",
    "       flu_label = 0  # Non-Fluent\n",
    "       com_label = 1  # Not Comprehends\n",
    "   \n",
    "   else:\n",
    "       return None, None, None, None, None, None\n",
    "\n",
    "   return type, status, ct_label, wab_label, type_label, flu_label, com_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1012/1012 [02:29<00:00,  6.78it/s]\n"
     ]
    }
   ],
   "source": [
    "video_col = ['NOSE_x', 'NOSE_y', 'NOSE_z', 'LEFT_EYE_INNER_x','LEFT_EYE_INNER_y', 'LEFT_EYE_INNER_z', 'LEFT_EYE_x', 'LEFT_EYE_y','LEFT_EYE_z', \n",
    "             'LEFT_EYE_OUTER_x', 'LEFT_EYE_OUTER_y','LEFT_EYE_OUTER_z', 'RIGHT_EYE_INNER_x', 'RIGHT_EYE_INNER_y','RIGHT_EYE_INNER_z', \n",
    "             'RIGHT_EYE_x', 'RIGHT_EYE_y', 'RIGHT_EYE_z','RIGHT_EYE_OUTER_x', 'RIGHT_EYE_OUTER_y', 'RIGHT_EYE_OUTER_z','LEFT_EAR_x', \n",
    "             'LEFT_EAR_y', 'LEFT_EAR_z', 'RIGHT_EAR_x', 'RIGHT_EAR_y','RIGHT_EAR_z', 'MOUTH_LEFT_x', 'MOUTH_LEFT_y', 'MOUTH_LEFT_z',\n",
    "             'MOUTH_RIGHT_x', 'MOUTH_RIGHT_y', 'MOUTH_RIGHT_z', 'LEFT_SHOULDER_x','LEFT_SHOULDER_y', 'LEFT_SHOULDER_z', 'RIGHT_SHOULDER_x',\n",
    "             'RIGHT_SHOULDER_y', 'RIGHT_SHOULDER_z', 'LEFT_ELBOW_x', 'LEFT_ELBOW_y','LEFT_ELBOW_z', 'RIGHT_ELBOW_x', 'RIGHT_ELBOW_y', \n",
    "             'RIGHT_ELBOW_z','LEFT_WRIST_x', 'LEFT_WRIST_y', 'LEFT_WRIST_z', 'RIGHT_WRIST_x','RIGHT_WRIST_y', 'RIGHT_WRIST_z', 'LEFT_PINKY_x', \n",
    "             'LEFT_PINKY_y','LEFT_PINKY_z', 'RIGHT_PINKY_x', 'RIGHT_PINKY_y', 'RIGHT_PINKY_z','LEFT_INDEX_x', 'LEFT_INDEX_y', 'LEFT_INDEX_z',\n",
    "             'RIGHT_INDEX_x','RIGHT_INDEX_y', 'RIGHT_INDEX_z', 'LEFT_THUMB_x', 'LEFT_THUMB_y','LEFT_THUMB_z', 'RIGHT_THUMB_x', 'RIGHT_THUMB_y','RIGHT_THUMB_z']\n",
    "\n",
    "audio_col = ['F0semitoneFrom27.5Hz_sma3nz_amean','F1amplitudeLogRelF0_sma3nz_amean','F1bandwidth_sma3nz_amean','F1frequency_sma3nz_amean',\n",
    "             'F2amplitudeLogRelF0_sma3nz_amean','F2bandwidth_sma3nz_amean','F2frequency_sma3nz_amean','F3amplitudeLogRelF0_sma3nz_amean',\n",
    "             'F3bandwidth_sma3nz_amean','F3frequency_sma3nz_amean','HNRdBACF_sma3nz_amean','alphaRatioV_sma3nz_amean',\n",
    "             'hammarbergIndexV_sma3nz_amean','jitterLocal_sma3nz_amean','logRelF0-H1-A3_sma3nz_amean','logRelF0-H1-H2_sma3nz_amean',\n",
    "             'loudness_sma3_amean','mfcc1_sma3_amean','mfcc2_sma3_amean','mfcc3_sma3_amean','mfcc4_sma3_amean','shimmerLocaldB_sma3nz_amean',\n",
    "             'slopeV0-500_sma3nz_amean','slopeV500-1500_sma3nz_amean','spectralFlux_sma3_amean']\n",
    "\n",
    "with open('D:/aphasia/MMATD/data_preprocessing/metadata.json','r') as f:\n",
    "    previous_metadata = json.load(f)\n",
    "    metadata = {}\n",
    "    for k,v in previous_metadata.items():\n",
    "        metadata[k.split('/')[1]] = v\n",
    "\n",
    "dataset_chunk = {\"user_name\": {}, 'status': {}, 'chunk_id': {}, \n",
    "                 \"type\": {}, \"sex\": {}, \"asr_body_pre\": {}, \n",
    "                 \"ct_label\": {}, \"wab_label\": {}, \"type_label\": {},\n",
    "                 \"flu_label\": {}, \"com_label\": {}, \"data_id\": {},\n",
    "                 \"duration\": {}}\n",
    "\n",
    "dirs = glob('D:/aphasia/dataset/tokens/text_bind/*')\n",
    "dfs = []\n",
    "audio_feats =[]\n",
    "\n",
    "\n",
    "data_id = 0\n",
    "for dir in tqdm(dirs):\n",
    "    # aling the features\n",
    "    txt_img_path = os.path.join(dir,'txt_img_paths.json')\n",
    "    opensmile_path = os.path.join(dir.replace('text_bind','opensmile_bind'),'txt_img_paths.json')\n",
    "    pose_path = os.path.join(dir.replace('text_bind','pose_bind'),'txt_img_paths.json')\n",
    "\n",
    "    row = {}\n",
    "    row['txt_img_path'] = txt_img_path \n",
    "    row['opensmile_path'] = opensmile_path\n",
    "    row['pose_path'] = pose_path\n",
    "\n",
    "    df_txt = pd.read_json(row['txt_img_path'])\n",
    "    df_aud = pd.read_json(row['opensmile_path'])[audio_col].interpolate()\n",
    "    df_vid = pd.read_json(row['pose_path'])[video_col].interpolate()\n",
    "\n",
    "    df_txt = pd.concat([df_txt,df_vid],axis=1)\n",
    "    df_txt = pd.concat([df_txt,df_aud],axis=1)\n",
    "\n",
    "    fn = df_txt.iloc[0]['token_img_path'].split('\\\\')[1]\n",
    "    metadata_info = metadata[fn]\n",
    "    aphasia_type, status, ct_label, wab_label, type_label, flu_label, com_label = get_labels(metadata_info['aphasia_type'])\n",
    "\n",
    "    if aphasia_type is None:\n",
    "        continue\n",
    "    # extract information for dataset_chunk\n",
    "    for i in range(0, len(df_txt), 50):\n",
    "        with open(f'D:/aphasia/dataset/transcripts/{fn}.json', 'r') as f:\n",
    "            transcript = json.load(f)\n",
    "        try:\n",
    "            if i + 50 <= len(df_txt):\n",
    "                dfs.append(df_txt.iloc[i:i+50])\n",
    "            else:\n",
    "                continue\n",
    "            data_id += 1\n",
    "            dataset_chunk['duration'][data_id] = transcript['chunks'][i//50]['end'] - transcript['chunks'][i//50]['start']\n",
    "            asr_body_pre = ' '.join(df_txt.iloc[i:i+50]['token'])\n",
    "\n",
    "            dataset_chunk['data_id'][data_id] = data_id\n",
    "            dataset_chunk['user_name'][data_id] = fn\n",
    "\n",
    "            dataset_chunk['chunk_id'][data_id] = i % 50\n",
    "            dataset_chunk['asr_body_pre'][data_id] = ' '.join(df_txt.iloc[i:i+50]['token']).replace('\\u2014', ' ')\n",
    "        \n",
    "\n",
    "            dataset_chunk['sex'][data_id] = metadata_info['sex']\n",
    "            dataset_chunk['data_id'][data_id] = data_id\n",
    "\n",
    "            dataset_chunk['status'][data_id] = status\n",
    "            dataset_chunk['type'][data_id] = aphasia_type\n",
    "            dataset_chunk['ct_label'][data_id] = ct_label\n",
    "            dataset_chunk['wab_label'][data_id] = wab_label\n",
    "            dataset_chunk['type_label'][data_id] = type_label\n",
    "            dataset_chunk['flu_label'][data_id] = flu_label\n",
    "            dataset_chunk['com_label'][data_id] = com_label\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # TODO: df_txt들 리스트에서 하나씩 꺼낼 때 또 50개 단위로 청킹하고 각 청크마다 인접행렬 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/aphasia/MMATD/dataset/dataset_chunk50.json','w') as f:\n",
    "    json.dump(dataset_chunk,f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8066/8066 [00:10<00:00, 792.31it/s]\n"
     ]
    }
   ],
   "source": [
    "def add_special_tokens_to_features(features):\n",
    "    _, feature_dim = features.shape\n",
    "    \n",
    "    cls_token = np.random.normal(0, 0.02, (1, feature_dim))\n",
    "    eos_token = np.random.normal(0, 0.02, (1, feature_dim))\n",
    "    \n",
    "    augmented_features = np.concatenate([cls_token, features, eos_token], axis=0)\n",
    "    \n",
    "    return augmented_features\n",
    "\n",
    "audio_feats_with_additional_tokens = []\n",
    "video_feats_with_additional_tokens = []\n",
    "audio_feats = []\n",
    "video_feats = []\n",
    "chunk_size = 50\n",
    "\n",
    "for df in tqdm(dfs):\n",
    "    audio_feat = np.array(df[audio_col].values)\n",
    "    video_feat = np.array(df[video_col].values)\n",
    "    audio_feats.append(audio_feat)\n",
    "    video_feats.append(video_feat)\n",
    "\n",
    "    audio_feats_with_additional_tokens.append(add_special_tokens_to_features(audio_feat))\n",
    "    video_feats_with_additional_tokens.append(add_special_tokens_to_features(video_feat))\n",
    "\n",
    "audio_feats = np.stack(audio_feats)\n",
    "video_feats = np.stack(video_feats)\n",
    "audio_feats_with_additional_tokens = np.stack(audio_feats_with_additional_tokens)\n",
    "video_feats_with_additional_tokens = np.stack(video_feats_with_additional_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('D:/aphasia/MMATD/dataset/opensmile_chunk52_feat.npy', audio_feats_with_additional_tokens)\n",
    "np.save('D:/aphasia/MMATD/dataset/pose_chunk52_feat.npy', video_feats_with_additional_tokens)\n",
    "np.save('D:/aphasia/MMATD/dataset/opensmile_chunk50_feat.npy', audio_feats)\n",
    "np.save('D:/aphasia/MMATD/dataset/pose_chunk50_feat.npy', video_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aphasia Type\n",
      "Control:  3364\n",
      "Anomic:  1871\n",
      "Conduction:  1171\n",
      "Wernicke:  439\n",
      "Sensory:  3\n",
      "Broca:  1110\n",
      "Motor:  42\n",
      "\n",
      "Subjects\n",
      "Control:  318\n",
      "Anomic:  247\n",
      "Conduction:  127\n",
      "Wernicke:  56\n",
      "Sensory:  1\n",
      "Broca:  186\n",
      "Motor:  11\n",
      "\n",
      "Duration\n",
      "Control:  19.850912604042808\n",
      "Anomic:  34.72995724211651\n",
      "Conduction:  27.382083689154566\n",
      "Wernicke:  28.12956719817768\n",
      "Sensory:  19.253333333333334\n",
      "Broca:  38.360369369369366\n",
      "Motor:  45.804761904761904\n",
      "Total:  27.59163625\n"
     ]
    }
   ],
   "source": [
    "print('Aphasia Type')\n",
    "print('Control: ', len([v for v in dataset_chunk['type'].values() if v == 'Control']))\n",
    "print('Anomic: ', len([v for v in dataset_chunk['type'].values() if v == 'Anomic']))\n",
    "print('Conduction: ', len([v for v in dataset_chunk['type'].values() if v == 'Conduction']))\n",
    "print('Wernicke: ', len([v for v in dataset_chunk['type'].values() if v == 'Wernicke']))\n",
    "print('Sensory: ', len([v for v in dataset_chunk['type'].values() if v == 'Trans. Sensory']))\n",
    "print('Broca: ', len([v for v in dataset_chunk['type'].values() if v == 'Broca']))\n",
    "print('Motor: ', len([v for v in dataset_chunk['type'].values() if v == 'Trans. Motor']))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Subjects')\n",
    "print('Control: ', len(np.unique([v for k, v in dataset_chunk['user_name'].items() if dataset_chunk['type'][k] == 'Control'])))\n",
    "print('Anomic: ', len(np.unique([v for k, v in dataset_chunk['user_name'].items() if dataset_chunk['type'][k] == 'Anomic'])))\n",
    "print('Conduction: ', len(np.unique([v for k, v in dataset_chunk['user_name'].items() if dataset_chunk['type'][k] == 'Conduction'])))\n",
    "print('Wernicke: ', len(np.unique([v for k, v in dataset_chunk['user_name'].items() if dataset_chunk['type'][k] == 'Wernicke'])))\n",
    "print('Sensory: ', len(np.unique([v for k, v in dataset_chunk['user_name'].items() if dataset_chunk['type'][k] == 'Trans. Sensory'])))\n",
    "print('Broca: ', len(np.unique([v for k, v in dataset_chunk['user_name'].items() if dataset_chunk['type'][k] == 'Broca'])))\n",
    "print('Motor: ', len(np.unique([v for k, v in dataset_chunk['user_name'].items() if dataset_chunk['type'][k] == 'Trans. Motor'])))  \n",
    "\n",
    "print()\n",
    "\n",
    "print('Duration')\n",
    "print('Control: ', np.mean([v for k, v in dataset_chunk['duration'].items() if dataset_chunk['type'][k] == 'Control']))\n",
    "print('Anomic: ', np.mean([v for k, v in dataset_chunk['duration'].items() if dataset_chunk['type'][k] == 'Anomic']))\n",
    "print('Conduction: ', np.mean([v for k, v in dataset_chunk['duration'].items() if dataset_chunk['type'][k] == 'Conduction']))\n",
    "print('Wernicke: ', np.mean([v for k, v in dataset_chunk['duration'].items() if dataset_chunk['type'][k] == 'Wernicke']))\n",
    "print('Sensory: ', np.mean([v for k, v in dataset_chunk['duration'].items() if dataset_chunk['type'][k] == 'Trans. Sensory']))  \n",
    "print('Broca: ', np.mean([v for k, v in dataset_chunk['duration'].items() if dataset_chunk['type'][k] == 'Broca']))\n",
    "print('Motor: ', np.mean([v for k, v in dataset_chunk['duration'].items() if dataset_chunk['type'][k] == 'Trans. Motor']))  \n",
    "print('Total: ', np.mean([v for k, v in dataset_chunk['duration'].items()]))                                                                                                 \n",
    "                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Anomic', 'Broca', 'Conduction', 'Control', 'Trans. Motor',\n",
       "       'Trans. Sensory', 'Wernicke'], dtype='<U14')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(list(dataset_chunk['type'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating all dataframes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating token masks: 100%|██████████| 300/300 [00:41<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating co-occurrence matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing features: 100%|██████████| 94/94 [00:33<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total video changes detected: 1866673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing features: 100%|██████████| 94/94 [00:00<00:00, 3470.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Co-occurrence statistics:\n",
      "Total tokens processed: 294098\n",
      "Total video changes: 1866673\n",
      "Total audio significant changes: 2614170\n",
      "\n",
      "Value distribution after adding min_value (0.01):\n",
      "Mean: 0.0101\n",
      "Min: 0.0100\n",
      "Max: 0.0195\n",
      "\n",
      "Token statistics:\n",
      "[*]: 63195 occurrences\n",
      "  - Avg video co-occurrence: 0.0130\n",
      "  - Avg audio co-occurrence: 0.0159\n",
      "the: 21527 occurrences\n",
      "  - Avg video co-occurrence: 0.0107\n",
      "  - Avg audio co-occurrence: 0.0124\n",
      "and: 22026 occurrences\n",
      "  - Avg video co-occurrence: 0.0111\n",
      "  - Avg audio co-occurrence: 0.0122\n",
      "to: 10846 occurrences\n",
      "  - Avg video co-occurrence: 0.0102\n",
      "  - Avg audio co-occurrence: 0.0112\n",
      "um: 462 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "she: 10053 occurrences\n",
      "  - Avg video co-occurrence: 0.0104\n",
      "  - Avg audio co-occurrence: 0.0113\n",
      "a: 7431 occurrences\n",
      "  - Avg video co-occurrence: 0.0102\n",
      "  - Avg audio co-occurrence: 0.0108\n",
      "her: 6111 occurrences\n",
      "  - Avg video co-occurrence: 0.0102\n",
      "  - Avg audio co-occurrence: 0.0106\n",
      "was: 4702 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0105\n",
      "they: 5265 occurrences\n",
      "  - Avg video co-occurrence: 0.0102\n",
      "  - Avg audio co-occurrence: 0.0106\n",
      "so: 4515 occurrences\n",
      "  - Avg video co-occurrence: 0.0103\n",
      "  - Avg audio co-occurrence: 0.0105\n",
      "that: 3788 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0104\n",
      "of: 4273 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0105\n",
      "cinderella: 2772 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0102\n",
      "it: 2653 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0103\n",
      "uh: 515 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "in: 2646 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0103\n",
      "i: 2975 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0103\n",
      "he: 2823 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0103\n",
      "but: 2682 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0103\n",
      "all: 2591 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0103\n",
      "is: 2348 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0102\n",
      "had: 2183 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0102\n",
      "then: 2075 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0102\n",
      "ball: 1110 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "go: 1794 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0102\n",
      "with: 2049 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0102\n",
      "prince: 1338 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "this: 2038 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0102\n",
      "one: 1420 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "on: 1573 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0102\n",
      "you: 2068 occurrences\n",
      "  - Avg video co-occurrence: 0.0101\n",
      "  - Avg audio co-occurrence: 0.0102\n",
      "two: 1235 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "there: 1202 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "were: 1312 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "for: 1465 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "going: 1268 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "king: 322 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "be: 1234 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "know: 719 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "out: 1205 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "at: 1179 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "no: 630 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "not: 1128 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "like: 1010 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "slipper: 596 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "get: 1112 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "have: 1239 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "dress: 863 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "got: 1086 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "up: 811 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "very: 948 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "went: 991 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "who: 927 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "glass: 831 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "because: 1044 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "or: 794 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "time: 547 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "stepmother: 744 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "oh: 1001 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "do: 903 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "into: 885 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "what: 864 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "beautiful: 832 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "back: 868 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "its: 15 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "home: 477 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "girl: 482 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "woman: 533 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "fairy: 754 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "when: 698 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "godmother: 627 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "would: 711 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "said: 450 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "just: 790 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "as: 835 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "little: 741 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "has: 830 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "shoe: 391 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "well: 231 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "his: 648 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "mother: 504 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "them: 496 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "daughters: 337 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "house: 453 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "stepsisters: 515 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "midnight: 299 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "by: 602 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "men: 35 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "bye: 62 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "girls: 374 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "mice: 471 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "sisters: 479 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "everything: 496 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "fit: 345 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "came: 505 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "other: 543 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "goes: 511 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "really: 499 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "find: 608 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "see: 521 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "man: 312 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "made: 587 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "about: 560 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "are: 600 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "off: 446 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "their: 509 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "okay: 98 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "pumpkin: 371 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "good: 394 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "nine: 9 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "could: 461 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "from: 405 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "dance: 266 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "can: 624 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "carriage: 319 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "away: 273 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "wanted: 439 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "found: 386 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "young: 355 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "gets: 398 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "father: 354 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "god: 110 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "around: 334 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "three: 235 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "think: 364 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "did: 333 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "slippers: 222 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "try: 364 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "nice: 607 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "thing: 250 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "castle: 247 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "mean: 351 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "down: 355 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "where: 371 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "day: 203 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "we: 321 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "married: 218 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "say: 202 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "tried: 289 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "looking: 320 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "come: 437 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "horses: 220 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "animals: 254 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "now: 246 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "left: 292 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "way: 235 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "12: 230 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "after: 229 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "too: 177 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "make: 351 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "anyway: 61 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "pretty: 268 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "some: 335 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "princess: 182 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "foot: 204 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "him: 282 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "something: 204 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "kind: 274 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "want: 384 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "right: 211 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "an: 281 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "work: 196 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "my: 361 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "big: 1466 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0101\n",
      "will: 293 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "comes: 284 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "things: 204 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "course: 175 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "yeah: 58 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "clock: 198 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "turned: 254 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "party: 136 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "people: 282 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "put: 325 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "lost: 223 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "over: 230 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "here: 206 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "shoes: 160 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "love: 240 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "if: 272 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "only: 248 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "lady: 169 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "saw: 233 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "how: 309 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "dancing: 184 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "took: 268 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "leave: 199 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "family: 121 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "guess: 152 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "yes: 88 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "told: 241 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "coach: 134 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "sudden: 169 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "room: 107 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "ran: 202 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "before: 198 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "wife: 109 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "kingdom: 170 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "friends: 230 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "take: 244 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "daughter: 135 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "story: 267 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "remember: 184 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "everybody: 230 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "trying: 264 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "clothes: 129 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "new: 214 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "gown: 102 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "ready: 184 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "dressed: 178 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "which: 203 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "look: 248 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "me: 210 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "lived: 144 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "evil: 164 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "danced: 172 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "also: 135 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "decided: 173 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "looked: 219 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "running: 166 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "happy: 141 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "says: 145 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "women: 160 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "help: 201 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "person: 175 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "son: 129 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "old: 174 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "horse: 175 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "stuff: 133 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "wear: 110 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "able: 148 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "fits: 108 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "wants: 170 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "maybe: 134 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "been: 152 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "these: 233 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "feet: 121 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "having: 177 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "invited: 106 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "magic: 157 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "even: 188 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "much: 156 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "clean: 121 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "excited: 113 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "again: 57 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "place: 133 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "ding: 232 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "wonderful: 121 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "night: 99 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "every: 142 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "invitation: 120 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "live: 140 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "finally: 101 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "thought: 129 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "marry: 121 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "once: 138 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "gave: 113 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "cleaning: 87 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "must: 129 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "turn: 170 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "care: 124 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "palace: 78 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "dad: 107 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "more: 109 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "bad: 116 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "sad: 60 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "being: 116 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "getting: 173 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "died: 71 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "name: 76 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "began: 30 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "step-sisters: 45 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "became: 121 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "does: 124 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "life: 72 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "servant: 87 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "met: 136 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "while: 129 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "first: 148 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "together: 89 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "step: 233 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "sure: 112 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "book: 92 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "children: 55 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n",
      "doing: 148 occurrences\n",
      "  - Avg video co-occurrence: 0.0100\n",
      "  - Avg audio co-occurrence: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_co_occurrence_matrix(dfs, disfluency_tokens, video_col, audio_col, std_multiplier=1.0, min_value=0.01):\n",
    "    \"\"\"\n",
    "    비디오와 오디오 특성을 따로 정규화하여 co-occurrence 계산\n",
    "    Args:\n",
    "        dfs: 데이터프레임 리스트\n",
    "        disfluency_tokens: 분석할 토큰 리스트\n",
    "        video_col: 비디오 특성 컬럼 리스트\n",
    "        audio_col: 오디오 특성 컬럼 리스트\n",
    "        std_multiplier: 표준편차의 몇 배를 threshold로 사용할지 결정하는 값\n",
    "        min_value: 모든 co-occurrence 값에 더할 최솟값\n",
    "    \"\"\"\n",
    "    print(\"Concatenating all dataframes...\")\n",
    "    combined_df = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # 토큰 마스크 생성\n",
    "    token_masks = {}\n",
    "    for token in tqdm(disfluency_tokens, desc=\"Creating token masks\"):\n",
    "        token_masks[token] = (combined_df['token'].str.lower() == token)\n",
    "        \n",
    "    # 각 토큰의 총 등장 횟수 계산\n",
    "    token_counts = {\n",
    "        token: mask.sum() \n",
    "        for token, mask in token_masks.items()\n",
    "    }\n",
    "    \n",
    "    print(\"Calculating co-occurrence matrix...\")\n",
    "    co_occurrences = {\n",
    "        'video': defaultdict(dict),\n",
    "        'audio': defaultdict(dict)\n",
    "    }\n",
    "    \n",
    "    with tqdm(total=len(video_col) + len(audio_col), desc=\"Processing features\") as pbar:\n",
    "        # 비디오 특성 처리\n",
    "        video_occurrences = 0\n",
    "        for col in video_col:\n",
    "            if col in combined_df.columns:\n",
    "                # 변화량 계산\n",
    "                changes = np.abs(combined_df[col].diff())\n",
    "                \n",
    "                # 변화량의 표준편차 계산\n",
    "                changes_std = changes.std()\n",
    "                if changes_std > 0:\n",
    "                    # 표준편차의 일정 배수를 threshold로 사용\n",
    "                    threshold = changes_std * std_multiplier\n",
    "                    changes_mask = (changes > threshold)\n",
    "                    \n",
    "                    video_occurrences += np.sum(changes_mask)\n",
    "                    \n",
    "                    for token in disfluency_tokens:\n",
    "                        co_occurrences['video'][col][token] = np.sum(\n",
    "                            token_masks[token] & changes_mask\n",
    "                        )\n",
    "                else:\n",
    "                    # 변화가 없는 경우\n",
    "                    print(f\"\\nWarning: No variation in feature '{col}'\")\n",
    "                    for token in disfluency_tokens:\n",
    "                        co_occurrences['video'][col][token] = 0\n",
    "                        \n",
    "                pbar.update(1)\n",
    "        \n",
    "        # 오디오 특성 처리 (기존과 동일)\n",
    "        audio_occurrences = 0\n",
    "        for col in audio_col:\n",
    "            if col in combined_df.columns:\n",
    "                mean = combined_df[col].mean()\n",
    "                std = combined_df[col].std()\n",
    "                if std > 0:\n",
    "                    z_scores = np.abs((combined_df[col] - mean) / std)\n",
    "                    z_score_mask = (z_scores > 1.0)\n",
    "                    audio_occurrences += np.sum(z_score_mask)\n",
    "                    \n",
    "                    for token in disfluency_tokens:\n",
    "                        co_occurrences['audio'][col][token] = np.sum(\n",
    "                            token_masks[token] & z_score_mask\n",
    "                        )\n",
    "                pbar.update(1)\n",
    "    \n",
    "    print(f\"\\nTotal video changes detected: {video_occurrences}\")\n",
    "    \n",
    "    # 정규화 부분 (기존과 동일)\n",
    "    with tqdm(total=len(video_col) + len(audio_col), desc=\"Normalizing features\") as pbar:\n",
    "        normalized_co_occurrences = {}\n",
    "        \n",
    "        # 비디오 특성 정규화 및 최솟값 더하기\n",
    "        for col in video_col:\n",
    "            normalized_co_occurrences[col] = {}\n",
    "            for token in disfluency_tokens:\n",
    "                if video_occurrences > 0:\n",
    "                    value = co_occurrences['video'][col][token] / video_occurrences\n",
    "                    normalized_co_occurrences[col][token] = value + min_value\n",
    "                else:\n",
    "                    normalized_co_occurrences[col][token] = min_value\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # 오디오 특성 정규화 및 최솟값 더하기\n",
    "        for col in audio_col:\n",
    "            normalized_co_occurrences[col] = {}\n",
    "            for token in disfluency_tokens:\n",
    "                if audio_occurrences > 0:\n",
    "                    value = co_occurrences['audio'][col][token] / audio_occurrences\n",
    "                    normalized_co_occurrences[col][token] = value + min_value\n",
    "                else:\n",
    "                    normalized_co_occurrences[col][token] = min_value\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # 통계 출력 (기존과 동일)\n",
    "    all_values = [\n",
    "        normalized_co_occurrences[col][token]\n",
    "        for col in normalized_co_occurrences\n",
    "        for token in disfluency_tokens\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nCo-occurrence statistics:\")\n",
    "    print(f\"Total tokens processed: {sum(token_counts.values())}\")\n",
    "    print(f\"Total video changes: {video_occurrences}\")\n",
    "    print(f\"Total audio significant changes: {audio_occurrences}\")\n",
    "    print(f\"\\nValue distribution after adding min_value ({min_value}):\")\n",
    "    print(f\"Mean: {np.mean(all_values):.4f}\")\n",
    "    print(f\"Min: {np.min(all_values):.4f}\")\n",
    "    print(f\"Max: {np.max(all_values):.4f}\")\n",
    "    \n",
    "    print(\"\\nToken statistics:\")\n",
    "    for token in disfluency_tokens:\n",
    "        if token_counts[token] > 0:\n",
    "            print(f\"{token}: {token_counts[token]} occurrences\")\n",
    "            video_avg = np.mean([normalized_co_occurrences[col][token] for col in video_col])\n",
    "            audio_avg = np.mean([normalized_co_occurrences[col][token] for col in audio_col])\n",
    "            print(f\"  - Avg video co-occurrence: {video_avg:.4f}\")\n",
    "            print(f\"  - Avg audio co-occurrence: {audio_avg:.4f}\")\n",
    "    \n",
    "    return normalized_co_occurrences\n",
    "\n",
    "\n",
    "disfluency_tokens = json.load(open('D:/aphasia/MMATD/dataset/_disfluency_tk_300.json', 'r'))\n",
    "\n",
    "co_occurrences = calculate_co_occurrence_matrix(\n",
    "    dfs, \n",
    "    disfluency_tokens, \n",
    "    video_col, \n",
    "    audio_col,\n",
    "    std_multiplier=1.5  # 표준편차의 1배를 threshold로 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating adjacency matrices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8066 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8066/8066 [08:28<00:00, 15.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix shape: (8066, 94, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_feature_adjacency_matrices(dfs, disfluency_tokens, video_col, audio_col, co_occurrences):\n",
    "    n_samples = len(dfs)\n",
    "    n_columns = len(video_col) + len(audio_col)\n",
    "    n_disfluency = len(disfluency_tokens)\n",
    "    \n",
    "    final_matrix = np.zeros((n_samples, n_columns, n_disfluency), dtype=np.float16)\n",
    "    \n",
    "    print(\"\\nCreating adjacency matrices...\")\n",
    "    for df_idx, df in enumerate(tqdm(dfs)):\n",
    "        token_indices = np.array([\n",
    "            disfluency_tokens.index(token.lower()) if token.lower() in disfluency_tokens else -1 \n",
    "            for token in df['token']\n",
    "        ])\n",
    "        valid_token_mask = token_indices != -1\n",
    "        \n",
    "        # 비디오 특성 처리\n",
    "        changes = np.zeros((len(df), len(video_col)))\n",
    "        for j, col in enumerate(video_col):\n",
    "            if col in df.columns:\n",
    "                diff = np.abs(df[col].diff().fillna(0))\n",
    "                changes[:, j] = (diff > 0).astype(np.float32)\n",
    "        \n",
    "        for j, col in enumerate(video_col):\n",
    "            if col in df.columns:\n",
    "                change_mask = changes[:, j] > 0\n",
    "                valid_positions = valid_token_mask & change_mask\n",
    "                if np.any(valid_positions):\n",
    "                    token_idx = token_indices[valid_positions]\n",
    "                    for pos, tok_idx in zip(np.where(valid_positions)[0], token_idx):\n",
    "                        final_matrix[df_idx, j, tok_idx] = co_occurrences[col][disfluency_tokens[tok_idx]]\n",
    "        \n",
    "        # 오디오 특성 처리\n",
    "        for j, col in enumerate(audio_col, start=len(video_col)):\n",
    "            if col in df.columns:\n",
    "                values = df[col].values\n",
    "                if len(values[~np.isnan(values)]) > 0:  # NaN이 아닌 값이 있는 경우에만 처리\n",
    "                    mean = np.nanmean(values)\n",
    "                    std = np.nanstd(values)\n",
    "                    \n",
    "                    if std > 0:  # 표준편차가 0보다 큰 경우에만 z-score 계산\n",
    "                        z_scores = np.abs((values - mean) / std)\n",
    "                        significant_mask = (z_scores > 1.0) & ~np.isnan(z_scores)\n",
    "                        \n",
    "                        valid_positions = valid_token_mask & significant_mask\n",
    "                        if np.any(valid_positions):\n",
    "                            token_idx = token_indices[valid_positions]\n",
    "                            for pos, tok_idx in zip(np.where(valid_positions)[0], token_idx):\n",
    "                                final_matrix[df_idx, j, tok_idx] = co_occurrences[col][disfluency_tokens[tok_idx]]\n",
    "    \n",
    "    return final_matrix\n",
    "\n",
    "adj_matrix = create_feature_adjacency_matrices(dfs, disfluency_tokens, video_col, audio_col, co_occurrences)\n",
    "print(f\"Adjacency matrix shape: {adj_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('D:/aphasia/MMATD/dataset/adj_chunk50_300_duration.npy', adj_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aphasia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
